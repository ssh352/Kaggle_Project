summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
library(tree)
library(ISLR)
help(Carseats)
attach(Carseats)   ###
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
tree.carseats
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
table(tree.pred, High.test)
(60 + 42)/120
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(53 + 33)/120
library(MASS)
help(Boston)
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
library(randomForest)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
?ramdonForest
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
rf.boston
fit$mse
oob.err = numeric(13)
oob.err
oob.err[mtry] = fit$mse[500]
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
importance(rf.boston)
varImpPlot(rf.boston)
importance(rf.boston, type = 1)
varImpPlot(rf.boston, type = 1)
?randomForest
install.packages("gbm")
library(gbm)
?gbm
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
par(mfrow = c(1, 1))
summary(boost.boston)
?gbm
par(mfrow = c(1, 2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
par(mfrow = c(1, 1))
summary(boost.boston)
summary(boost.boston)
par(mfrow = c(1, 2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
dim(predmat)
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(oob.err), col = "red")
predmat - medv
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
medv
predmat
Boston$medv
dim(predmat)
n.trees = seq(from = 100, to = 100000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
n.trees = seq(from = 100, to = 20000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
n.trees = seq(from = 100, to = 11000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
dim(predmat)
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
?gbm
iris.meas = iris[, -5]
summary(iris.meas)
sapply(iris.meas, sd)
iris.scale = as.data.frame(scale(iris.meas))
summary(iris.scale)
sapply(iris.scale, sd)
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Scaled Iris Data")
set.seed(0)
km.iris = kmeans(iris.scale, centers = 3)
km.iris
par(mfrow = c(1, 2))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "True Species", col = iris$Species)
par(mfrow = c(1, 1))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
points(km.iris$centers[, 4], km.iris$centers[, 2], pch = 16, col = "blue")
wssplot = function(data, nc = 15, seed = 0) {
wss = (nrow(data) - 1) * sum(apply(data, 2, var))
for (i in 2:nc) {
set.seed(seed)
wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)
}
plot(1:nc, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within-Cluster Variance",
main = "Scree Plot for the K-Means Procedure")
}
wssplot(iris.scale)
faithful.scale = scale(faithful)
summary(faithful.scale)
?faithful
par(mfrow = c(1, 1))
plot(faithful.scale)
wssplot(faithful.scale)
set.seed(0)
km.faithful1 = kmeans(faithful.scale, centers = 3) #Running the K-means procedure
km.faithful2 = kmeans(faithful.scale, centers = 3) #5 different times, but with
km.faithful3 = kmeans(faithful.scale, centers = 3) #only one convergence of the
km.faithful4 = kmeans(faithful.scale, centers = 3) #algorithm each time.
km.faithful5 = kmeans(faithful.scale, centers = 3)
set.seed(0)
km.faithfulsim = kmeans(faithful.scale, centers = 3, nstart = 100)
par(mfrow = c(2, 3))
plot(faithful, col = km.faithful1$cluster,
main = paste("Single K-Means Attempt #1\n WCV: ",
round(km.faithful1$tot.withinss, 4)))
plot(faithful, col = km.faithful2$cluster,
main = paste("Single K-Means Attempt #2\n WCV: ",
round(km.faithful2$tot.withinss, 4)))
plot(faithful, col = km.faithful3$cluster,
main = paste("Single K-Means Attempt #3\n WCV: ",
round(km.faithful3$tot.withinss, 4)))
plot(faithful, col = km.faithful4$cluster,
main = paste("Single K-Means Attempt #4\n WCV: ",
round(km.faithful4$tot.withinss, 4)))
plot(faithful, col = km.faithful5$cluster,
main = paste("Single K-Means Attempt #5\n WCV: ",
round(km.faithful5$tot.withinss, 4)))
plot(faithful, col = km.faithfulsim$cluster,
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.faithfulsim$tot.withinss, 4)))
library(ISLR)
data(OJ)
str(OJ)
model1 = tree(Purchase ~ ., split = "gini", data = OJ)
library(tree)
model1 = tree(Purchase ~ ., split = "gini", data = OJ)
train <- OJ[train_index, ]
test <- OJ[test_index, ]
model1 = tree(Purchase ~ ., split = "gini", data = train)
library(tree)
model1 = tree(Purchase ~ ., split = "gini", data = train)
train <- OJ[train_index, ]
test <- OJ[test_index, ]
set.seed(0)
train_index <- sample(1:nrow(OJ), 8*nrow(OJ)/10)
test_index <- -train_index
train <- OJ[train_index, ]
test <- OJ[test_index, ]
library(tree)
model1 = tree(Purchase ~ ., split = "gini", data = train)
summary(model1)
1 - 0.1449
model1.pred <- predict(model1, test[, -1], type = "class")
table(model1.pred, test[, 1])
a <- c('a', 'b')
b <- c:2
b <- 1:2
table(a, b)
(106 + 59)/dim(OJ)[1]
dim(OJ)[1]
(106 + 59)/dim(test)[1]
set.seed(0)
cv.model2 <- cv.tree(model1, FUN = prune.misclass)
par(mfrow = c(1, 2))
plot(cv.model2$size, cv.model2$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.model2$k, cv.model2$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
names(cv.model2)
cv.model2
library(data.table)
library(Matrix)
library(xgboost)
library(Metrics)
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
install.packages("xgboost", repos = "http://dmlc.ml/drat/", type = "source")
setwd("C:/Users/Xinyuan Wu/Desktop/Xinyuan's Repo/Kaggle_Project")
train <- read.csv("data/train.csv/train.csv")
submit <- read.csv("data/test.csv/test.csv")
num_train <- train[, -c(1: 117, 132)]
num_submit <- submit[, -c(1:117)]
train_full_id <- train[, 1]
submit_id <- submit[, 1]
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(caret))
train <- train %>% select(-id)   ### remove id column
submit <- submit %>% select(-id)
cat_train <- train[, 1:116]
cat_submit <- submit[, 1:116]
set.seed(1314)
train_index <- sample(1:nrow(train), 8*nrow(train)/10)
test_index <- -train_index
train_id <- train_full_id[train_index]
test_id <- train_full_id[test_index]
submit2 <- submit
submit2$loss <- NA
submit2_cat <- submit2[, 1:116]
submit2_num <- submit2[, 117:131]
submit2_cat_conv <- data.frame(lapply(submit2_cat, as.character),
loss = submit2$loss, stringsAsFactors = FALSE)
train2 <- train
train2_cat <- train2[, 1:116]
train2_num <- train2[, 117:131]
train2_cat_conv <- data.frame(lapply(train2_cat, as.character),
loss = train2$loss, stringsAsFactors = FALSE)
full <- rbind(train2_cat_conv, submit2_cat_conv)
find_different <- function(x, y) {
return(c(setdiff(x, y), setdiff(y, x)))
}
filter_cat <- function(x, remove) {
return(mapvalues(x, remove, rep('ZZ', length(remove))))
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### before
TRUE & FALSE
length(unique(train2_cat_conv[, 1])) != length(unique(submit2_cat_conv[, 1]))
length(unique(train2_cat_conv[, 1])) != length(unique(submit2_cat_conv[, 1])) & length('a') > 0
length(unique(train2_cat_conv[, 1])) == length(unique(submit2_cat_conv[, 1])) & length('a') > 0
length(unique(train2_cat_conv[, 1])) != length(unique(submit2_cat_conv[, 1])) &
length('a') > 0
length(unique(train2_cat_conv[, 1])) == length(unique(submit2_cat_conv[, 1])) &
length('a') > 0
for (i in 1:116) {
remove <- find_different(train2_cat_conv[, i], submit2_cat_conv[, i])
if (length(unique(train2_cat_conv[, i])) != length(unique(submit2_cat_conv[, i])) &
length(remove) > 0) {
print(paste('========', i))
print(remove)
full[, i] <- filter_cat(full[, i], remove)
print('===================================================================')}
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### after
sum(sapply(full[, 1:116], function(x) sum(x == 'ZZ')))   ### after
submit2 <- submit
submit2$loss <- NA
submit2_cat <- submit2[, 1:116]
submit2_num <- submit2[, 117:131]
submit2_cat_conv <- data.frame(lapply(submit2_cat, as.character),
loss = submit2$loss, stringsAsFactors = FALSE)
train2 <- train
train2_cat <- train2[, 1:116]
train2_num <- train2[, 117:131]
train2_cat_conv <- data.frame(lapply(train2_cat, as.character),
loss = train2$loss, stringsAsFactors = FALSE)
full <- rbind(train2_cat_conv, submit2_cat_conv)
find_different <- function(x, y) {
return(c(setdiff(x, y), setdiff(y, x)))
}
filter_cat <- function(x, remove) {
return(mapvalues(x, remove, rep('ZZ', length(remove))))
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### before
sum(sapply(full[, 1:116], function(x) sum(x == 'ZZ')))   ### before
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### before
for (i in 1:116) {
remove <- find_different(train2_cat_conv[, i], submit2_cat_conv[, i])
if (length(remove) > 0) {
print(paste('========', i))
print(remove)
full[, i] <- filter_cat(full[, i], remove)
print('===================================================================')}
}
submit2 <- submit
submit2$loss <- NA
submit2_cat <- submit2[, 1:116]
submit2_num <- submit2[, 117:131]
submit2_cat_conv <- data.frame(lapply(submit2_cat, as.character),
loss = submit2$loss, stringsAsFactors = FALSE)
train2 <- train
train2_cat <- train2[, 1:116]
train2_num <- train2[, 117:131]
train2_cat_conv <- data.frame(lapply(train2_cat, as.character),
loss = train2$loss, stringsAsFactors = FALSE)
full <- rbind(train2_cat_conv, submit2_cat_conv)
find_different <- function(x, y) {
return(c(setdiff(x, y), setdiff(y, x)))
}
filter_cat <- function(x, remove) {
return(mapvalues(x, remove, rep('NA', length(remove))))
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### before
for (i in 1:116) {
remove <- find_different(train2_cat_conv[, i], submit2_cat_conv[, i])
if (length(remove) > 0) {
print(paste('========', i))
print(remove)
full[, i] <- filter_cat(full[, i], remove)
print('===================================================================')}
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### after
submit2 <- submit
submit2$loss <- NA
submit2_cat <- submit2[, 1:116]
submit2_num <- submit2[, 117:131]
submit2_cat_conv <- data.frame(lapply(submit2_cat, as.character),
loss = submit2$loss, stringsAsFactors = FALSE)
train2 <- train
train2_cat <- train2[, 1:116]
train2_num <- train2[, 117:131]
train2_cat_conv <- data.frame(lapply(train2_cat, as.character),
loss = train2$loss, stringsAsFactors = FALSE)
full <- rbind(train2_cat_conv, submit2_cat_conv)
find_different <- function(x, y) {
return(c(setdiff(x, y), setdiff(y, x)))
}
filter_cat <- function(x, remove) {
return(mapvalues(x, remove, rep('NA', length(remove))))
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### before
for (i in 1:116) {
remove <- find_different(train2_cat_conv[, i], submit2_cat_conv[, i])
if (length(unique(train2_cat_conv[, i])) != length(unique(submit2_cat_conv[, i])) &
length(remove) > 0) {
print(paste('========', i))
print(remove)
full[, i] <- filter_cat(full[, i], remove)
print('===================================================================')}
}
sum(sapply(full[, 1:116], function(x) sum(x == 'NA')))   ### after
as.numeric(factor(c('a', 'b'' NA)))
)
)
)))
)))
1
1
213213
asdfdsaf
)))))))))))))))))))
}
na
asd
''
as.numeric(factor(c('a', 'b', NA)))
levels(factor(c('Z', 'ZZZ')))
levels(factor(c('Z', 'ZZ')))
levels(factor(c('ZZ', 'ZZZ')))
as.numericfactor(c('ZZ', 'ZZZ'))
as.numeric(factor(c('ZZ', 'ZZZ')))
sum(sapply(full[, 1:116], function(x) sum(x == 'ZZZ')))   ### before
submit2 <- submit
submit2$loss <- NA
submit2_cat <- submit2[, 1:116]
submit2_num <- submit2[, 117:131]
submit2_cat_conv <- data.frame(lapply(submit2_cat, as.character),
loss = submit2$loss, stringsAsFactors = FALSE)
train2 <- train
train2_cat <- train2[, 1:116]
train2_num <- train2[, 117:131]
train2_cat_conv <- data.frame(lapply(train2_cat, as.character),
loss = train2$loss, stringsAsFactors = FALSE)
full <- rbind(train2_cat_conv, submit2_cat_conv)
find_different <- function(x, y) {
return(c(setdiff(x, y), setdiff(y, x)))
}
filter_cat <- function(x, remove) {
return(mapvalues(x, remove, rep('ZZZ', length(remove))))
}
sum(sapply(full[, 1:116], function(x) sum(x == 'ZZZ')))   ### before
for (i in 1:116) {
remove <- find_different(train2_cat_conv[, i], submit2_cat_conv[, i])
if (length(unique(train2_cat_conv[, i])) != length(unique(submit2_cat_conv[, i])) &
length(remove) > 0) {
print(paste('========', i))
print(remove)
full[, i] <- filter_cat(full[, i], remove)
print('===================================================================')}
}
sum(sapply(full[, 1:116], function(x) sum(x == 'ZZZ')))   ### after
full_factorize <- data.frame(lapply(full[, 1:116], function(x) as.numeric(factor(x))),
loss = full$loss, stringsAsFactors = FALSE)
str(full_factorize)
train_encoded2 <- cbind(train_full_id, full_factorize[!is.na(full_factorize$loss), 1:116], train2_num)
str(train_encoded2)
str(train)
train_encoded2 <- cbind(id = train_full_id,
full_factorize[!is.na(full_factorize$loss), 1:116],
train2_num)
str(train_encoded2)
submit_encoded2 <- cbind(id = submit_id,
full_factorize[is.na(full_factorize$loss), 1:116],
submit2[, 117:130])
str(submit_encoded2)
train_version2 <- train_encoded2[train_index, ]
test_version2 <- train_encoded2[test_index, ]
submit_version2 <- submit_encoded2
dim(train_version2)
dim(test_version2)
dim(submit_version2)
write.csv(train_version2, file = 'train_encode2_v2.csv', row.names = F)
write.csv(test_version2, file = 'test_encode2_v2.csv', row.names = F)
write.csv(train_encoded2, file = 'train_full_encode2_v2.csv', row.names = F)
write.csv(submit_version2, file = 'submit_encode2_v2.csv', row.names = F)
unique(train$cat116)
sapply(train[, 1:116], function(x) length(unique(x)))
unique(train$cat110)
unique(train$cat112)
